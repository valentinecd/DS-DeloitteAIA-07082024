{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecting Pipelines with AWS\n",
    "##### [Source](https://docs.aws.amazon.com/sagemaker/latest/dg/define-pipeline.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** *This lecture presupposes an adequate [AWS Overview](./AWS%20Overview.md). Additionally, it is assumed that the [Pipeline Execution notebook](./Pipeline%20Execution.ipynb) content will be discussed at the beginning of the [Pipeline Execution lecture](https://github.com/flatiron-school/DS-Deloitte-07062022-Pipeline-Execution-on-AWS).*\n",
    "\n",
    "To orchestrate your workflows with Amazon SageMaker Model Building Pipelines, you need to generate a directed acyclic graph (DAG) in the form of a JSON pipeline definition. The following image is a representation of the pipeline DAG that is created using the workflow described in this notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/pipeline-full.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Pipelines\n",
    "\n",
    "SageMaker Pipelines supports the following activities, which are demonstrated in this notebook:\n",
    "\n",
    "* Pipelines - A DAG of steps and conditions to orchestrate SageMaker jobs and resource creation.\n",
    "\n",
    "* Processing job steps - A simplified, managed experience on SageMaker to run data processing workloads, such as feature engineering, data validation, model evaluation, and model interpretation.\n",
    "\n",
    "* Training job steps - An iterative process that teaches a model to make predictions by presenting examples from a training dataset.\n",
    "\n",
    "* Conditional execution steps - A step that provides conditional execution of branches in a pipeline.\n",
    "\n",
    "* Register model steps - A step that creates a model package resource in the Model Registry that can be used to create deployable models in Amazon SageMaker.\n",
    "\n",
    "* Create model steps - A step that creates a model for use in transform steps or later publication as an endpoint.\n",
    "\n",
    "* Transform job steps - A batch transform to preprocess datasets to remove noise or bias that interferes with training or inference from a dataset, get inferences from large datasets, and run inference when a persistent endpoint is not needed.\n",
    "\n",
    "* Fail steps - A step that stops a pipeline execution and marks the pipeline execution as failed.\n",
    "\n",
    "* Parametrized Pipeline executions - Enables variation in pipeline executions according to specified parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'toc'></a>\n",
    "## Notebook Overview\n",
    "\n",
    "This notebook shows how to:\n",
    "\n",
    "* [Define a set of Pipeline parameters](#pipeline-parameters) that can be used to parametrize a SageMaker Pipeline.\n",
    "\n",
    "* [Define a Processing step that performs cleaning](#processing-step), feature engineering, and splitting the input data into train and test data sets.\n",
    "\n",
    "* [Define a Training step](#training-step) that trains a model on the preprocessed train data set.\n",
    "\n",
    "* [Define a Processing step that evaluates the trained model](#processing-step-2)’s performance on the test dataset.\n",
    "\n",
    "* [Define a Create Model step that creates a model](#create-model-step) from the model artifacts used in training.\n",
    "\n",
    "* [Define a Transform step that performs batch transformation](#batch-transform-step) based on the model that was created.\n",
    "\n",
    "* [Define a Register Model step](#register-model-step) that creates a model package from the estimator and model artifacts used to train the model.\n",
    "\n",
    "* [Define a Conditional step](#condition-step) that measures a condition based on output from prior steps and conditionally executes other steps.\n",
    "\n",
    "* [Define a Fail step with a customized error message](#fail-step) indicating the cause of the execution failure.\n",
    "\n",
    "* [Define and create a Pipeline](#create-pipeline-step) definition in a DAG, with the defined parameters and steps.\n",
    "\n",
    "<!-- * Start a Pipeline execution and wait for execution to complete.\n",
    "\n",
    "* Download the model evaluation report from the S3 bucket for examination. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A SageMaker Pipeline\n",
    "\n",
    "The pipeline that you create follows a typical machine learning (ML) application pattern of preprocessing, training, evaluation, model creation, batch transformation, and model registration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/pipeline-full1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "###### To run the following tutorial you must do the following:\n",
    "\n",
    "* Set up your notebook instance as outlined in [Create a notebook instance](https://docs.aws.amazon.com/sagemaker/latest/dg/howitworks-create-ws.html). This gives your role permissions to read and write to Amazon S3, and create training, batch transform, and processing jobs in SageMaker.\n",
    "* (Optional) Upload and run the \"Quick Start\" notebook provided in the `sagemaker_upload` folder ([Link](https://github.com/flatiron-school/DS-Deloitte-07062022-Architecting-Pipelines-with-AWS/tree/main/sagemaker_upload)) by following the instructions in the `README`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Your Environment\n",
    "Create a new SageMaker session using the following code block. This returns the role ARN for the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sagemaker'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-9c0e774ebfdc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mboto3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mregion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mboto3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregion_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sagemaker'"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import sagemaker.session\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "model_package_group_name = f\"AbaloneModelPackageGroupName\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.227.0-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: requests in c:\\users\\cvalentine\\appdata\\local\\anaconda3\\envs\\learn-env\\lib\\site-packages (from sagemaker) (2.24.0)\n",
      "Collecting psutil\n",
      "  Downloading psutil-6.0.0-cp37-abi3-win_amd64.whl (257 kB)\n",
      "Collecting attrs<24,>=23.1.0\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Collecting schema\n",
      "  Downloading schema-0.7.7-py2.py3-none-any.whl (18 kB)\n",
      "Collecting cloudpickle==2.2.1\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\cvalentine\\appdata\\local\\anaconda3\\envs\\learn-env\\lib\\site-packages (from sagemaker) (1.1.3)\n",
      "Collecting protobuf<5.0,>=3.12\n",
      "  Downloading protobuf-4.25.4-cp38-cp38-win_amd64.whl (413 kB)\n",
      "Collecting boto3<2.0,>=1.34.142\n",
      "  Downloading boto3-1.34.151-py3-none-any.whl (139 kB)\n",
      "Collecting urllib3<3.0.0,>=1.26.8\n",
      "  Downloading urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\cvalentine\\appdata\\local\\anaconda3\\envs\\learn-env\\lib\\site-packages (from sagemaker) (3.2.0)\n",
      "Collecting pathos\n",
      "  Downloading pathos-0.3.2-py3-none-any.whl (82 kB)\n",
      "Requirement already satisfied: google-pasta in c:\\users\\cvalentine\\appdata\\local\\anaconda3\\envs\\learn-env\\lib\\site-packages (from sagemaker) (0.2.0)\n",
      "Collecting tblib<4,>=1.7.0\n",
      "  Downloading tblib-3.0.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in c:\\users\\cvalentine\\appdata\\local\\anaconda3\\envs\\learn-env\\lib\\site-packages (from sagemaker) (1.18.5)\n",
      "Collecting platformdirs\n",
      "  Downloading platformdirs-4.2.2-py3-none-any.whl (18 kB)\n",
      "Collecting smdebug-rulesconfig==1.0.1\n",
      "  Downloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in c:\\users\\cvalentine\\appdata\\local\\anaconda3\\envs\\learn-env\\lib\\site-packages (from sagemaker) (2.0.0)\n",
      "Collecting PyYAML~=6.0\n",
      "  Downloading PyYAML-6.0.1-cp38-cp38-win_amd64.whl (157 kB)\n",
      "Collecting docker\n",
      "  Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\cvalentine\\appdata\\local\\anaconda3\\envs\\learn-env\\lib\\site-packages (from sagemaker) (4.50.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\cvalentine\\appdata\\local\\anaconda3\\envs\\learn-env\\lib\\site-packages (from sagemaker) (20.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\cvalentine\\appdata\\local\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests->sagemaker) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\cvalentine\\appdata\\local\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests->sagemaker) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cvalentine\\appdata\\local\\anaconda3\\envs\\learn-env\\lib\\site-packages (from requests->sagemaker) (2020.6.20)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\cvalentine\\appdata\\local\\anaconda3\\envs\\learn-env\\lib\\site-packages (from pandas->sagemaker) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\cvalentine\\appdata\\local\\anaconda3\\envs\\learn-env\\lib\\site-packages (from pandas->sagemaker) (2.8.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\cvalentine\\appdata\\local\\anaconda3\\envs\\learn-env\\lib\\site-packages (from boto3<2.0,>=1.34.142->sagemaker) (0.10.0)\n",
      "Collecting botocore<1.35.0,>=1.34.151\n",
      "  Downloading botocore-1.34.151-py3-none-any.whl (12.4 MB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0\n",
      "  Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\cvalentine\\appdata\\local\\anaconda3\\envs\\learn-env\\lib\\site-packages (from jsonschema->sagemaker) (1.15.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\cvalentine\\appdata\\local\\anaconda3\\envs\\learn-env\\lib\\site-packages (from jsonschema->sagemaker) (50.3.0.post20201103)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\cvalentine\\appdata\\local\\anaconda3\\envs\\learn-env\\lib\\site-packages (from jsonschema->sagemaker) (0.17.3)\n",
      "Collecting dill>=0.3.8\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Collecting multiprocess>=0.70.16\n",
      "  Downloading multiprocess-0.70.16-py38-none-any.whl (132 kB)\n",
      "Collecting pox>=0.3.4\n",
      "  Downloading pox-0.3.4-py3-none-any.whl (29 kB)\n",
      "Collecting ppft>=1.7.6.8\n",
      "  Downloading ppft-1.7.6.8-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\cvalentine\\appdata\\local\\anaconda3\\envs\\learn-env\\lib\\site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.3.0)\n",
      "Collecting pywin32>=304; sys_platform == \"win32\"\n",
      "  Downloading pywin32-306-cp38-cp38-win_amd64.whl (9.4 MB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\cvalentine\\appdata\\local\\anaconda3\\envs\\learn-env\\lib\\site-packages (from packaging>=20.0->sagemaker) (2.4.7)\n",
      "Installing collected packages: psutil, attrs, schema, cloudpickle, protobuf, urllib3, botocore, s3transfer, boto3, dill, multiprocess, pox, ppft, pathos, tblib, platformdirs, smdebug-rulesconfig, PyYAML, pywin32, docker, sagemaker\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 20.2.0\n",
      "    Uninstalling attrs-20.2.0:\n",
      "      Successfully uninstalled attrs-20.2.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.11.2\n",
      "    Uninstalling protobuf-3.11.2:\n",
      "      Successfully uninstalled protobuf-3.11.2\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.10\n",
      "    Uninstalling urllib3-1.25.10:\n",
      "      Successfully uninstalled urllib3-1.25.10\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.18.16\n",
      "    Uninstalling botocore-1.18.16:\n",
      "      Successfully uninstalled botocore-1.18.16\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.3.3\n",
      "    Uninstalling s3transfer-0.3.3:\n",
      "      Successfully uninstalled s3transfer-0.3.3\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.15.16\n",
      "    Uninstalling boto3-1.15.16:\n",
      "      Successfully uninstalled boto3-1.15.16\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 5.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot uninstall 'PyYAML'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Pipeline\n",
    "\n",
    "Run the following steps from your SageMaker notebook instance to create a pipeline including steps for preprocessing, training, evaluation, conditional evaluation, and model registration.\n",
    "\n",
    "### Step 1: Download the Dataset\n",
    "\n",
    "This notebook uses the [UCI Machine Learning Abalone Dataset](http://archive.ics.uci.edu/ml). The dataset contains the following features:\n",
    "\n",
    "* `length` – The longest shell measurement of the abalone.\n",
    "\n",
    "* `diameter` – The diameter of the abalone perpendicular to its length.\n",
    "\n",
    "* `height` – The height of the abalone with meat in the shell.\n",
    "\n",
    "* `whole_weight` – The weight of the whole abalone.\n",
    "\n",
    "* `shucked_weight` – The weight of the meat removed from the abalone.\n",
    "\n",
    "* `viscera_weight` – The weight of the abalone viscera after bleeding.\n",
    "\n",
    "* `shell_weight` – The weight of the abalone shell after meat removal and drying.\n",
    "\n",
    "* `sex` – The sex of the abalone. One of 'M', 'F', or 'I', where 'I' is an infant abalone.\n",
    "\n",
    "* `rings` – The number of rings in the abalone shell.\n",
    "\n",
    "The number of rings in the abalone shell is a good approximation for its age using the formula `age = rings + 1.5`. However, obtaining this number is a time-consuming task. You must cut the shell through the cone, stain the section, and count the number of rings through a microscope. However, the other physical measurements are easier to determine. This notebook uses the dataset to build a predictive model of the variable rings using the other physical measurements.\n",
    "\n",
    "#### To download the dataset\n",
    "\n",
    "- Download the dataset into your account's default Amazon S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data\n",
    "local_path = \"data/abalone-dataset.csv\"\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "s3.Bucket(f\"sagemaker-servicecatalog-seedcode-{region}\").download_file(\n",
    "    \"dataset/abalone-dataset.csv\",\n",
    "    local_path\n",
    ")\n",
    "\n",
    "base_uri = f\"s3://{default_bucket}/abalone\"\n",
    "input_data_uri = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=local_path, \n",
    "    desired_s3_uri=base_uri,\n",
    ")\n",
    "print(input_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download a second dataset for batch transformation after your model is created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \"data/abalone-dataset-batch\"\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "s3.Bucket(f\"sagemaker-servicecatalog-seedcode-{region}\").download_file(\n",
    "    \"dataset/abalone-dataset-batch\",\n",
    "    local_path\n",
    ")\n",
    "\n",
    "base_uri = f\"s3://{default_bucket}/abalone\"\n",
    "batch_data_uri = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=local_path, \n",
    "    desired_s3_uri=base_uri,\n",
    ")\n",
    "print(batch_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to TOC](#toc)\n",
    "<a id = 'pipeline-parameters'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define Pipeline Parameters\n",
    "\n",
    "![](images/pipeline-1.png)\n",
    "\n",
    "Define Pipeline parameters that you can use to parametrize the pipeline. Parameters enable custom pipeline executions and schedules without having to modify the Pipeline definition.\n",
    "\n",
    "The supported parameter types include:\n",
    "\n",
    "* `ParameterString` - represents a str Python type\n",
    "\n",
    "* `ParameterInteger` - represents an int Python type\n",
    "\n",
    "* `ParameterFloat` - represents a float Python type\n",
    "\n",
    "These parameters support providing a default value, which can be overridden on pipeline execution. The default value specified should be an instance of the type of the parameter.\n",
    "\n",
    "This code block defines the following parameters for your pipeline:\n",
    "\n",
    "* `processing_instance_count` – The instance count of the processing job.\n",
    "\n",
    "* `input_data` – The Amazon S3 location of the input data.\n",
    "\n",
    "* `batch_data` – The Amazon S3 location of the input data for batch transformation.\n",
    "\n",
    "* `model_approval_status` – The approval status to register the trained model with for CI/CD. For more information, see [Automate MLOps with SageMaker Projects](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    ")\n",
    "\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name=\"ProcessingInstanceCount\",\n",
    "    default_value=1\n",
    ")\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\",\n",
    "    default_value=\"PendingManualApproval\"\n",
    ")\n",
    "input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=input_data_uri,\n",
    ")\n",
    "batch_data = ParameterString(\n",
    "    name=\"BatchData\",\n",
    "    default_value=batch_data_uri,\n",
    ")\n",
    "mse_threshold = ParameterFloat(name=\"MseThreshold\", default_value=6.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to TOC](#toc)\n",
    "<a id = 'processing-step'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define a Processing Step for Feature Engineering\n",
    "\n",
    "![](images/pipeline-2.png)\n",
    "\n",
    "This section shows how to create a processing step to prepare the data from the dataset for training.\n",
    "\n",
    "#### To create a processing step\n",
    "\n",
    "- Create a directory for the processing script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p abalone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a file in the `/abalone` directory named `preprocessing.py` with the following content. This preprocessing script is passed in to the processing step for execution on the input data. The training step then uses the preprocessed training features and labels to train a model, and the evaluation step uses the trained model and preprocessed test features and labels to evaluate the model. The script uses `scikit-learn` to do the following:\n",
    "\n",
    "    * Fill in missing `sex` categorical data and encode it so it's suitable for training.\n",
    "\n",
    "    * Scale and normalize all numerical fields except for `rings` and `sex`.\n",
    "\n",
    "    * Split the data into training, test, and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile abalone/preprocessing.py\n",
    "import argparse\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Because this is a headerless CSV file, specify the column names here.\n",
    "feature_columns_names = [\n",
    "    \"sex\",\n",
    "    \"length\",\n",
    "    \"diameter\",\n",
    "    \"height\",\n",
    "    \"whole_weight\",\n",
    "    \"shucked_weight\",\n",
    "    \"viscera_weight\",\n",
    "    \"shell_weight\",\n",
    "]\n",
    "label_column = \"rings\"\n",
    "\n",
    "feature_columns_dtype = {\n",
    "    \"sex\": str,\n",
    "    \"length\": np.float64,\n",
    "    \"diameter\": np.float64,\n",
    "    \"height\": np.float64,\n",
    "    \"whole_weight\": np.float64,\n",
    "    \"shucked_weight\": np.float64,\n",
    "    \"viscera_weight\": np.float64,\n",
    "    \"shell_weight\": np.float64\n",
    "}\n",
    "label_column_dtype = {\"rings\": np.float64}\n",
    "\n",
    "def merge_two_dicts(x, y):\n",
    "    z = x.copy()\n",
    "    z.update(y)\n",
    "    return z\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = \"/opt/ml/processing\"\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        f\"{base_dir}/input/abalone-dataset.csv\",\n",
    "        header=None, \n",
    "        names=feature_columns_names + [label_column],\n",
    "        dtype=merge_two_dicts(feature_columns_dtype, label_column_dtype)\n",
    "    )\n",
    "    numeric_features = list(feature_columns_names)\n",
    "    numeric_features.remove(\"sex\")\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    categorical_features = [\"sex\"]\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"cat\", categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    y = df.pop(\"rings\")\n",
    "    X_pre = preprocess.fit_transform(df)\n",
    "    y_pre = y.to_numpy().reshape(len(y), 1)\n",
    "    \n",
    "    X = np.concatenate((y_pre, X_pre), axis=1)\n",
    "    \n",
    "    np.random.shuffle(X)\n",
    "    train, validation, test = np.split(X, [int(.7*len(X)), int(.85*len(X))])\n",
    "\n",
    "    pd.DataFrame(train).to_csv(f\"{base_dir}/train/train.csv\", header=False, index=False)\n",
    "    pd.DataFrame(validation).to_csv(f\"{base_dir}/validation/validation.csv\", header=False, index=False)\n",
    "    pd.DataFrame(test).to_csv(f\"{base_dir}/test/test.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create an instance of an `SKLearnProcessor` to pass in to the processing step. \n",
    "\n",
    "    Note the `processing_instance_count` parameter used by the processor instance (as defined above in **Step 2**). Also previously-specified was the `framework_version` to use throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    "    ParameterBoolean\n",
    ")\n",
    "\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name=\"ProcessingInstanceCount\",\n",
    "    default_value=1\n",
    ")\n",
    "\n",
    "framework_version = \"0.23-1\"\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"sklearn-abalone-process\",\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a processing step. This step takes in the `SKLearnProcessor`, the input and output channels, and the `preprocessing.py` script that you created. This is very similar to a processor instance's run method in the SageMaker Python SDK. The `input_data` parameter passed into `ProcessingStep` is the input data of the step itself. This input data is used by the processor instance when it runs.\n",
    "\n",
    "    Note the `\"train\"`, `\"validation\"`, and `\"test\"` channels specified in the output configuration for the processing job. Step `Properties` such as these can be used in subsequent steps and resolve to their runtime values at execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "    \n",
    "step_process = ProcessingStep(\n",
    "    name=\"AbaloneProcess\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "      ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),  \n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\")\n",
    "    ],\n",
    "    code=\"abalone/preprocessing.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to TOC](#toc)\n",
    "<a id = 'training-step'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Define a Training step\n",
    "\n",
    "![](images/pipeline-3.png)\n",
    "\n",
    "This section shows how to use the SageMaker [XGBoost Algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) to train a logistic regression model on the training data output from the processing steps.\n",
    "\n",
    "#### To define a training step\n",
    "\n",
    "- Specify the model path where you want to save the models from training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"s3://{default_bucket}/AbaloneTrain\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Configure an estimator for the XGBoost algorithm and the input dataset. The training instance type is passed into the estimator. A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to `model_dir` so that it can be hosted later. SageMaker uploads the model to Amazon S3 in the form of a `model.tar.gz` at the end of the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.0-1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=\"ml.m5.xlarge\"\n",
    ")\n",
    "\n",
    "xgb_train = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    output_path=model_path,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "xgb_train.set_hyperparameters(\n",
    "    objective=\"reg:linear\",\n",
    "    num_round=50,\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.7,\n",
    "    silent=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a `TrainingStep` using the estimator instance and properties of the `ProcessingStep`. In particular, pass in the `S3Uri` of the `\"train\"` and `\"validation\"` output channel to the `TrainingStep`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name=\"AbaloneTrain\",\n",
    "    estimator=xgb_train,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"validation\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to TOC](#toc)\n",
    "<a id = 'processing-step-2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Define a Processing Step for Model Evaluation\n",
    "\n",
    "![](images/pipeline-4.png)\n",
    "\n",
    "This section shows how to create a processing step to evaluate the accuracy of the model. The result of this model evaluation is used in the condition step to determine which execute path to take.\n",
    "\n",
    "#### To define a processing step for model evaluation\n",
    "\n",
    "- Create a file in the `/abalone` directory named `evaluation.py`. This script is used in a processing step to perform model evaluation. It takes a trained model and the test dataset as input, then produces a JSON file containing classification evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile abalone/evaluation.py\n",
    "import json\n",
    "import pathlib\n",
    "import pickle\n",
    "import tarfile\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = f\"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "    \n",
    "    model = pickle.load(open(\"xgboost-model\", \"rb\"))\n",
    "\n",
    "    test_path = \"/opt/ml/processing/test/test.csv\"\n",
    "    df = pd.read_csv(test_path, header=None)\n",
    "    \n",
    "    y_test = df.iloc[:, 0].to_numpy()\n",
    "    df.drop(df.columns[0], axis=1, inplace=True)\n",
    "    \n",
    "    X_test = xgboost.DMatrix(df.values)\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    std = np.std(y_test - predictions)\n",
    "    report_dict = {\n",
    "        \"regression_metrics\": {\n",
    "            \"mse\": {\n",
    "                \"value\": mse,\n",
    "                \"standard_deviation\": std\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    evaluation_path = f\"{output_dir}/evaluation.json\"\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        f.write(json.dumps(report_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create an instance of a `ScriptProcessor` that is used to create a `ProcessingStep`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "script_eval = ScriptProcessor(\n",
    "    image_uri=image_uri,\n",
    "    command=[\"python3\"],\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    base_job_name=\"script-abalone-eval\",\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a `ProcessingStep` using the processor instance, the input and output channels, and the `evaluation.py` script. In particular, pass in the `S3ModelArtifacts` property from the `step_train` training step, as well as the `S3Uri` of the `\"test\"` output channel of the `step_process` processing step. This is very similar to a processor instance's `run` method in the SageMaker Python SDK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\"\n",
    ")\n",
    "step_eval = ProcessingStep(\n",
    "    name=\"AbaloneEval\",\n",
    "    processor=script_eval,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\"\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"test\"\n",
    "            ].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "    ],\n",
    "    code=\"abalone/evaluation.py\",\n",
    "    property_files=[evaluation_report],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to TOC](#toc)\n",
    "<a id = 'create-model-step'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Define a CreateModelStep for Batch Transformation\n",
    "\n",
    "![](images/pipeline-5.png)\n",
    "\n",
    "This section shows how to create a SageMaker model from the output of the training step. This model is used for batch transformation on a new dataset. This step is passed into the condition step and only executes if the condition step evaluates to `true`.\n",
    "\n",
    "#### To define a CreateModelStep for batch transformation\n",
    "\n",
    "- Create a SageMaker model. Pass in the `S3ModelArtifacts` property from the `step_train` training step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "model = Model(\n",
    "    image_uri=image_uri,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define the model input for your SageMaker model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import CreateModelInput\n",
    "\n",
    "inputs = CreateModelInput(\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    accelerator_type=\"ml.eia1.medium\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create your `CreateModelStep` using the `CreateModelInput` and SageMaker `model` instance you defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "\n",
    "step_create_model = CreateModelStep(\n",
    "    name=\"AbaloneCreateModel\",\n",
    "    model=model,\n",
    "    inputs=inputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to TOC](#toc)\n",
    "<a id = 'batch-transform-step'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Define a TransformStep to Perform Batch Transformation\n",
    "\n",
    "This section shows how to create a `TransformStep` to perform batch transformation on a dataset after the model is trained. This step is passed into the condition step and only executes if the condition step evaluates to `true`.\n",
    "\n",
    "#### To define a TransformStep to perform batch transformation\n",
    "\n",
    "- Create a transformer instance with the appropriate compute instance type, instance count, and desired output Amazon S3 bucket URI. Pass in the `ModelName` property from the `step_create_model` `CreateModel` step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "transformer = Transformer(\n",
    "    model_name=step_create_model.properties.ModelName,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    output_path=f\"s3://{default_bucket}/AbaloneTransform\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a `TransformStep` using the transformer instance you defined and the `batch_data` pipeline parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TransformInput\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "\n",
    "step_transform = TransformStep(\n",
    "    name=\"AbaloneTransform\",\n",
    "    transformer=transformer,\n",
    "    inputs=TransformInput(data=batch_data)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to TOC](#toc)\n",
    "<a id = 'register-model-step'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Define a RegisterModel Step to Create a Model Package\n",
    "\n",
    "This section shows how to construct an instance of `RegisterModel`. The result of executing `RegisterModel` in a pipeline is a model package. A model package is a reusable model artifacts abstraction that packages all ingredients necessary for inference. It consists of an inference specification that defines the inference image to use along with an optional model weights location. A model package group is a collection of model packages. You can use a `ModelPackageGroup` for SageMaker Pipelines to add a new version and model package to the group for every pipeline execution. For more information about model registry, see [Register and Deploy Models with Model Registry](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html).\n",
    "\n",
    "This step is passed into the condition step and only executes if the condition step evaluates to `true`.\n",
    "\n",
    "#### To define a RegisterModel step to create a model package\n",
    "\n",
    "* Construct a `RegisterModel` step using the estimator instance you used for the training step. Pass in the `S3ModelArtifacts` property from the `step_train` training step and specify a `ModelPackageGroup`. SageMaker Pipelines creates this `ModelPackageGroup` for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics \n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=\"{}/evaluation.json\".format(\n",
    "            step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "        ),\n",
    "        content_type=\"application/json\"\n",
    "    )\n",
    ")\n",
    "\n",
    "step_register = RegisterModel(\n",
    "    name=\"AbaloneRegisterModel\",\n",
    "    estimator=xgb_train,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to TOC](#toc)\n",
    "<a id = 'fail-step'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Define a Fail Step to Terminate the Pipeline Execution and Mark it as Failed\n",
    "\n",
    "![](images/pipeline-8.png)\n",
    "\n",
    "This section walks you through the following steps:\n",
    "\n",
    "* Define a FailStep with customized error message, which indicates the cause of the execution failure.\n",
    "\n",
    "* Enter the FailStep error message with a Join function, which appends a static text string with the dynamic mse_threshold parameter to build a more informative error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.fail_step import FailStep\n",
    "from sagemaker.workflow.functions import Join\n",
    "\n",
    "step_fail = FailStep(\n",
    "    name=\"AbaloneMSEFail\",\n",
    "    error_message=Join(on=\" \", values=[\"Execution failed due to MSE >\", mse_threshold]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to TOC](#toc)\n",
    "<a id = 'condition-step'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Define a Condition Step to Verify Model Accuracy\n",
    "\n",
    "![](images/pipeline-6.png)\n",
    "\n",
    "A `ConditionStep` allows SageMaker Pipelines to support conditional execution in your pipeline DAG based on the condition of step properties. In this case, you only want to register a model package if the accuracy of that model, as determined by the model evaluation step, exceeds the required value. If the accuracy exceeds the required value, the pipeline also creates a SageMaker Model and runs batch transformation on a dataset. This section shows how to define the Condition step.\n",
    "\n",
    "#### To define a condition step to verify model accuracy\n",
    "\n",
    "- Define a `ConditionLessThanOrEqualTo` condition using the accuracy value found in the output of the model evaluation processing step, `step_eval`. Get this output using the property file you indexed in the processing step and the respective JSONPath of the mean squared error value, `\"mse\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "cond_lte = ConditionLessThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=step_eval.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"regression_metrics.mse.value\"\n",
    "    ),\n",
    "    right=mse_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Construct a `ConditionStep`. Pass the `ConditionEquals` condition in, then set the model package registration and batch transformation steps as the next steps if the condition passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_cond = ConditionStep(\n",
    "    name=\"AbaloneMSECond\",\n",
    "    conditions=[cond_lte],\n",
    "    if_steps=[step_register, step_create_model, step_transform],\n",
    "    else_steps=[step_fail], \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to TOC](#toc)\n",
    "<a id = 'create-pipeline-step'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Create a pipeline\n",
    "\n",
    "![](images/pipeline-7.png)\n",
    "\n",
    "Now that you’ve created all of the steps, you can combine them into a pipeline!\n",
    "\n",
    "#### To create a pipeline\n",
    "\n",
    "- Define the following for your pipeline: `name`, `parameters`, and `steps`. Names must be unique within an (`account`, `region`) pair.\n",
    "\n",
    "**Note**: *A step can only appear once* in either the pipeline's step list or the if/else step lists of the condition step. It cannot appear in both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = f\"AbalonePipeline\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_count,\n",
    "        model_approval_status,\n",
    "        input_data,\n",
    "        batch_data,\n",
    "        mse_threshold,\n",
    "    ],\n",
    "    steps=[step_process, step_train, step_eval, step_cond],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (Optional) Examine the JSON pipeline definition to ensure that it's well-formed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.loads(pipeline.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline definition is ready to submit to SageMaker. During the next lecture, we'll submit this pipeline to SageMaker and [start an execution](https://github.com/flatiron-school/DS-Deloitte-07062022-Architecting-Pipelines-with-AWS/blob/main/Pipeline%20Execution.ipynb)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
